{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJy_Kyrvd60W"
   },
   "source": [
    "NOTE: Some of the results here will not necessarily match with the results from our Written Report due to random number generation with computation time and different test splits affecting accuracy but should the differences should be negligble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnU4q8OkemW0"
   },
   "source": [
    "Installing all important libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZARV582Riaf",
    "outputId": "68f0a5ae-5d65-440e-fe9c-0dd972692616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbeQuvsQh12c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, Input, Lambda, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6C80_hc5cKa"
   },
   "source": [
    "full thing with PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "O4kXZCP1VzOj",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# what does this do :\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv(\"METABRIC_RNA_Mutation.csv\")\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"age_at_diagnosis\", \"tumor_other_histologic_subtype\", \"inferred_menopausal_state\",\n",
    "    \"mutation_count\", \"hormone_therapy\", \"patient_id\", \"cellularity\",\n",
    "    \"type_of_breast_surgery\", \"cohort\", \"chemotherapy\", \"tumor_size\",\n",
    "    \"hormone_therapy\", \"menopausal_state\", \"overall_survival_months\",\n",
    "    \"overall_survival\", \"death_from_cancer\", \"tumor_stage\", \"er_status\",\n",
    "    \"pam50_+_claudin-low_subtype\", \"pr_status\", \"er_status_measured_by_ihc\",\n",
    "    \"nottingham_prognostic_index\", \"tumor_stage\", \"death_from_cancer\",\n",
    "    \"integrative_cluster\", \"her2_status_measured_by_snp6\", \"her2_status\",\n",
    "    \"radio_therapy\", \"3-gene_classifier_subtype\",   \"oncotree_code\"\n",
    "]\n",
    "\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(data[\"cancer_type_detailed\"].value_counts())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"cancer_type_detailed\"])\n",
    "\n",
    "X = pd.get_dummies(data.drop(columns=[\"cancer_type_detailed\"]), drop_first=True)\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "print(X_scaled.shape)\n",
    "\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "selected_featuresLasso = X.columns[(lasso.coef_ != 0)]\n",
    "\n",
    "print(\"Selected features:\", selected_featuresLasso)\n",
    "\n",
    "\n",
    "X=X[selected_featuresLasso]\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_selected = X[selected_featuresLasso]\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_selected_imputed = imputer.fit_transform(X_selected)\n",
    "\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "X_pca = pca.fit_transform(X_selected_imputed)\n",
    "\n",
    "# Output the number of components and transformed data shape\n",
    "print(\"Number of PCA components:\", pca.n_components_)\n",
    "print(\"Shape of PCA-transformed data:\", X_pca.shape)\n",
    "\n",
    "\n",
    "class_counts = Counter(y)\n",
    "single_instance_classes = [cls for cls, count in class_counts.items() if count < 2]\n",
    "\n",
    "# Create a mask to filter out these classes\n",
    "mask = ~np.isin(y, single_instance_classes)\n",
    "X_pca_filtered = X_pca[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "# Check the new class distribution after filtering\n",
    "print(\"Class distribution after filtering:\", Counter(y_filtered))\n",
    "\n",
    "# Now split the filtered data\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca_filtered, y_filtered, test_size=0.3, stratify=y_filtered, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_balanced))\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_folds = 5\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Re-encode y_train_balanced to ensure sequential class labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_balanced_encoded = label_encoder.fit_transform(y_train_balanced)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Array to store fold results\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "# Determine input shape dynamically\n",
    "input_shape = X_train_balanced.shape[1]\n",
    "num_classes = len(np.unique(y_train_balanced_encoded))  # Determine number of classes\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_balanced, y_train_balanced_encoded)):\n",
    "    print(f\"Training fold {fold+1}/{num_folds}...\")\n",
    "\n",
    "    # Split data\n",
    "    X_train_fold, X_val_fold = X_train_balanced[train_idx], X_train_balanced[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_balanced_encoded[train_idx], y_train_balanced_encoded[val_idx]\n",
    "\n",
    "    # Define the model (redefine for each fold to reset weights)\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001), input_shape=(input_shape,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer matches re-encoded classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=epochs, batch_size=batch_size, validation_data=(X_val_fold, y_val_fold), verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    fold_losses.append(val_loss)\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "    print(f\"Fold {fold+1} - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Report cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(f\"Mean Validation Loss: {np.mean(fold_losses):.4f} ± {np.std(fold_losses):.4f}\")\n",
    "print(f\"Mean Validation Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted class labels\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(\"Test F1 Score:\", f1)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_classes))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61vgR-TsiMlp"
   },
   "source": [
    "SMOTE Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Z15pB2_UVz_F",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"METABRIC_RNA_Mutation.csv\")\n",
    "\n",
    "# Define columns to drop\n",
    "columns_to_drop = [\n",
    "    \"age_at_diagnosis\", \"tumor_other_histologic_subtype\", \"inferred_menopausal_state\",\n",
    "    \"mutation_count\", \"hormone_therapy\", \"patient_id\", \"cellularity\",\n",
    "    \"type_of_breast_surgery\", \"cohort\", \"chemotherapy\", \"tumor_size\",\n",
    "    \"hormone_therapy\", \"menopausal_state\", \"overall_survival_months\",\n",
    "    \"overall_survival\", \"death_from_cancer\", \"tumor_stage\", \"er_status\",\n",
    "    \"pam50_+_claudin-low_subtype\", \"pr_status\", \"er_status_measured_by_ihc\",\n",
    "    \"nottingham_prognostic_index\", \"tumor_stage\", \"death_from_cancer\",\n",
    "    \"integrative_cluster\", \"her2_status_measured_by_snp6\", \"her2_status\",\n",
    "    \"radio_therapy\", \"3-gene_classifier_subtype\", \"oncotree_code\"\n",
    "]\n",
    "\n",
    "# Drop specified columns and encode the target variable\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"cancer_type_detailed\"])\n",
    "\n",
    "# One-hot encode features and impute missing values\n",
    "X = pd.get_dummies(data.drop(columns=[\"cancer_type_detailed\"]), drop_first=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "print(X_scaled.shape)\n",
    "\n",
    "# Filter out classes with fewer than two instances\n",
    "class_counts = Counter(y)\n",
    "single_instance_classes = [cls for cls, count in class_counts.items() if count < 2]\n",
    "mask = ~np.isin(y, single_instance_classes)\n",
    "X_filtered = X_scaled[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "# Split data and apply SMOTE to the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.3, stratify=y_filtered, random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_balanced))\n",
    "\n",
    "# Re-encode y_train_balanced and y_test to ensure sequential labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_balanced_encoded = label_encoder.fit_transform(y_train_balanced)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "num_classes = len(np.unique(y_train_balanced_encoded))\n",
    "\n",
    "# Set cross-validation and neural network parameters\n",
    "num_folds = 5\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Initialize arrays to store fold results\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "# Define Stratified K-Fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "input_shape = X_train_balanced.shape[1]\n",
    "\n",
    "# Cross-validation training\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_balanced, y_train_balanced_encoded)):\n",
    "    print(f\"Training fold {fold+1}/{num_folds}...\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_balanced[train_idx], X_train_balanced[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_balanced_encoded[train_idx], y_train_balanced_encoded[val_idx]\n",
    "\n",
    "    # Define and compile the neural network model\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001), input_shape=(input_shape,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=epochs, batch_size=batch_size, validation_data=(X_val_fold, y_val_fold), verbose=1)\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    fold_losses.append(val_loss)\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "    print(f\"Fold {fold+1} - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Report cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(f\"Mean Validation Loss: {np.mean(fold_losses):.4f} ± {np.std(fold_losses):.4f}\")\n",
    "print(f\"Mean Validation Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_classes)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "f1 = f1_score(y_test_encoded, y_pred_classes, average='weighted')\n",
    "print(\"Test F1 Score:\", f1)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_encoded, y_pred_classes))\n",
    "\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_encoded, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vqvwv1PG4GbG"
   },
   "source": [
    "Lasso and SMOTE Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "cSqbttkhfAUN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Data Loading and Cleaning\n",
    "data = pd.read_csv(\"METABRIC_RNA_Mutation.csv\", low_memory=False)\n",
    "columns_to_drop = [\"age_at_diagnosis\", \"tumor_other_histologic_subtype\", \"inferred_menopausal_state\",\n",
    "                   \"mutation_count\", \"hormone_therapy\", \"patient_id\", \"cellularity\",\n",
    "                   \"type_of_breast_surgery\", \"cohort\", \"chemotherapy\", \"tumor_size\",\n",
    "                   \"hormone_therapy\", \"menopausal_state\", \"overall_survival_months\",\n",
    "                   \"overall_survival\", \"death_from_cancer\", \"tumor_stage\", \"er_status\",\n",
    "                   \"pam50_+_claudin-low_subtype\", \"pr_status\", \"er_status_measured_by_ihc\",\n",
    "                   \"nottingham_prognostic_index\", \"tumor_stage\", \"death_from_cancer\",\n",
    "                   \"integrative_cluster\", \"her2_status_measured_by_snp6\", \"her2_status\",\n",
    "                   \"radio_therapy\", \"3-gene_classifier_subtype\", \"oncotree_code\"]\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"cancer_type_detailed\"])\n",
    "\n",
    "# Step 2: One-Hot Encoding & Imputation\n",
    "X = pd.get_dummies(data.drop(columns=[\"cancer_type_detailed\"]), drop_first=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Step 3: Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Step 4: Feature Selection with Lasso\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_scaled, y)\n",
    "selected_featuresLasso = X.columns[(lasso.coef_ != 0)]\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)[selected_featuresLasso]\n",
    "\n",
    "# Step 5: Class Filtering\n",
    "class_counts = Counter(y)\n",
    "mask = ~np.isin(y, [cls for cls, count in class_counts.items() if count < 2])\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "# Step 6: Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Step 7: Class Balancing with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Re-encode y_train_balanced to ensure sequential class labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_balanced = label_encoder.fit_transform(y_train_balanced)\n",
    "\n",
    "# Ensure X_train_balanced is in NumPy format\n",
    "if isinstance(X_train_balanced, pd.DataFrame):\n",
    "    X_train_balanced = X_train_balanced.to_numpy()\n",
    "\n",
    "# Step 8: Cross-Validation Training\n",
    "num_folds = 5\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 0.0005\n",
    "input_shape = X_train_balanced.shape[1]\n",
    "num_classes = len(np.unique(y_train_balanced))  # Determine number of classes\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies, fold_losses = [], []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_train_balanced, y_train_balanced):\n",
    "    X_train_fold, X_val_fold = X_train_balanced[train_idx], X_train_balanced[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_balanced[train_idx], y_train_balanced[val_idx]\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),  # Use Input layer as first layer\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=epochs, batch_size=batch_size, validation_data=(X_val_fold, y_val_fold), verbose=1)\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    fold_losses.append(val_loss)\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(f\"Mean Validation Loss: {np.mean(fold_losses):.4f} ± {np.std(fold_losses):.4f}\")\n",
    "print(f\"Mean Validation Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "# Step 9: Final Model Evaluation\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi5rpXHN6yN7"
   },
   "source": [
    "Autoencoder full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fyGBiuvY4n-T",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"METABRIC_RNA_Mutation.csv\")\n",
    "\n",
    "# Data Cleaning: Drop specified columns and encode target variable\n",
    "columns_to_drop = [\n",
    "    \"age_at_diagnosis\", \"tumor_other_histologic_subtype\", \"inferred_menopausal_state\",\n",
    "    \"mutation_count\", \"hormone_therapy\", \"patient_id\", \"cellularity\",\n",
    "    \"type_of_breast_surgery\", \"cohort\", \"chemotherapy\", \"tumor_size\",\n",
    "    \"hormone_therapy\", \"menopausal_state\", \"overall_survival_months\",\n",
    "    \"overall_survival\", \"death_from_cancer\", \"tumor_stage\", \"er_status\",\n",
    "    \"pam50_+_claudin-low_subtype\", \"pr_status\", \"er_status_measured_by_ihc\",\n",
    "    \"nottingham_prognostic_index\", \"tumor_stage\", \"death_from_cancer\",\n",
    "    \"integrative_cluster\", \"her2_status_measured_by_snp6\", \"her2_status\",\n",
    "    \"radio_therapy\", \"3-gene_classifier_subtype\", \"oncotree_code\"\n",
    "]\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"cancer_type_detailed\"])\n",
    "\n",
    "# One-Hot Encoding & Imputation\n",
    "X = pd.get_dummies(data.drop(columns=[\"cancer_type_detailed\"]), drop_first=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Feature Selection with Lasso\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_scaled, y)\n",
    "selected_featuresLasso = X.columns[(lasso.coef_ != 0)]\n",
    "X_selected = X_scaled[:, lasso.coef_ != 0]\n",
    "\n",
    "# Autoencoder for Dimensionality Reduction\n",
    "input_dim = X_selected.shape[1]\n",
    "encoding_dim = 50  # Set the encoding dimension (adjust based on desired reduction level)\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_selected, X_selected, epochs=50, batch_size=32, shuffle=True, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Extract the encoder part to get the compressed representation\n",
    "encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "X_encoded = encoder_model.predict(X_selected)\n",
    "\n",
    "# Class Filtering: Remove classes with fewer than 2 instances\n",
    "class_counts = Counter(y)\n",
    "single_instance_classes = [cls for cls, count in class_counts.items() if count < 2]\n",
    "mask = ~np.isin(y, single_instance_classes)\n",
    "X_filtered = X_encoded[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "# Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.3, stratify=y_filtered, random_state=42)\n",
    "\n",
    "# Class Balancing with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Cross-Validation Training\n",
    "num_folds = 5\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 0.0005\n",
    "y_train_balanced_encoded = LabelEncoder().fit_transform(y_train_balanced)\n",
    "input_shape = X_train_balanced.shape[1]\n",
    "num_classes = len(np.unique(y_train_balanced_encoded))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_balanced, y_train_balanced_encoded)):\n",
    "    print(f\"Training fold {fold+1}/{num_folds}...\")\n",
    "    X_train_fold, X_val_fold = X_train_balanced[train_idx], X_train_balanced[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_balanced_encoded[train_idx], y_train_balanced_encoded[val_idx]\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001), input_shape=(input_shape,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=epochs, batch_size=batch_size, validation_data=(X_val_fold, y_val_fold), verbose=1)\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    fold_losses.append(val_loss)\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "    print(f\"Fold {fold+1} - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Report cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(f\"Mean Validation Loss: {np.mean(fold_losses):.4f} ± {np.std(fold_losses):.4f}\")\n",
    "print(f\"Mean Validation Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "# Final Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(\"Test F1 Score:\", f1)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_classes))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_encoded, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6PsS2vi7bUw"
   },
   "source": [
    "VariationalAutoencoder full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2mbnhAWv6Sbv",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"METABRIC_RNA_Mutation.csv\", low_memory=False)\n",
    "\n",
    "# Data Cleaning: Drop specified columns and encode target variable\n",
    "columns_to_drop = [\n",
    "    \"age_at_diagnosis\", \"tumor_other_histologic_subtype\", \"inferred_menopausal_state\",\n",
    "    \"mutation_count\", \"hormone_therapy\", \"patient_id\", \"cellularity\",\n",
    "    \"type_of_breast_surgery\", \"cohort\", \"chemotherapy\", \"tumor_size\",\n",
    "    \"hormone_therapy\", \"menopausal_state\", \"overall_survival_months\",\n",
    "    \"overall_survival\", \"death_from_cancer\", \"tumor_stage\", \"er_status\",\n",
    "    \"pam50_+_claudin-low_subtype\", \"pr_status\", \"er_status_measured_by_ihc\",\n",
    "    \"nottingham_prognostic_index\", \"tumor_stage\", \"death_from_cancer\",\n",
    "    \"integrative_cluster\", \"her2_status_measured_by_snp6\", \"her2_status\",\n",
    "    \"radio_therapy\", \"3-gene_classifier_subtype\", \"oncotree_code\"\n",
    "]\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"cancer_type_detailed\"])\n",
    "\n",
    "# One-Hot Encoding & Imputation\n",
    "X = pd.get_dummies(data.drop(columns=[\"cancer_type_detailed\"]), drop_first=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Feature Selection with Lasso\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_scaled, y)\n",
    "selected_featuresLasso = X.columns[(lasso.coef_ != 0)]\n",
    "X_selected = X_scaled[:, lasso.coef_ != 0]\n",
    "\n",
    "if X_selected.shape[1] == 0:\n",
    "    print(\"Warning: Lasso selected 0 features. Using all features instead.\")\n",
    "    X_selected = X_scaled  # Use all features if Lasso selects none\n",
    "\n",
    "# VAE for Dimensionality Reduction\n",
    "input_dim = X_selected.shape[1]\n",
    "latent_dim = 50  # Latent space dimension for dimensionality reduction\n",
    "\n",
    "# Define VAE model\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Custom layer for KL divergence loss\n",
    "class KLDivergenceLayer(Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        self.add_loss(K.mean(kl_loss))\n",
    "        return z_mean\n",
    "\n",
    "# Encoder network\n",
    "inputs = Input(shape=(input_dim,))\n",
    "h = Dense(128, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "z_mean = KLDivergenceLayer()([z_mean, z_log_var])\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# Decoder network\n",
    "decoder_h = Dense(128, activation='relu')\n",
    "decoder_mean = Dense(input_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# Define the VAE model\n",
    "vae = Model(inputs, x_decoded_mean)\n",
    "\n",
    "# Custom VAE loss function for reconstruction only\n",
    "def vae_loss(inputs, x_decoded_mean):\n",
    "    reconstruction_loss = K.sum(K.square(inputs - x_decoded_mean), axis=-1)\n",
    "    return K.mean(reconstruction_loss)\n",
    "\n",
    "# Compile the VAE model with the custom loss\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "# Train VAE\n",
    "vae.fit(X_selected, X_selected, epochs=50, batch_size=32, shuffle=True, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Extract encoder part to get compressed latent space representation\n",
    "encoder = Model(inputs, z_mean)\n",
    "X_encoded = encoder.predict(X_selected)\n",
    "\n",
    "# Impute NaN values in X_encoded using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent'\n",
    "X_encoded = imputer.fit_transform(X_encoded)\n",
    "\n",
    "if X_encoded.shape[1] == 0:\n",
    "    print(\"Warning: X_encoded has 0 features after imputation. Using original features instead.\")\n",
    "    X_encoded = X_selected  # Use original features if X_encoded is empty\n",
    "\n",
    "# Class Filtering: Remove classes with fewer than 2 instances\n",
    "class_counts = Counter(y)\n",
    "single_instance_classes = [cls for cls, count in class_counts.items() if count < 2]\n",
    "mask = ~np.isin(y, single_instance_classes)\n",
    "X_filtered = X_encoded[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "# Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.3, stratify=y_filtered, random_state=42)\n",
    "\n",
    "# Class Balancing with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Cross-Validation Training\n",
    "num_folds = 5\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 0.0005\n",
    "y_train_balanced_encoded = LabelEncoder().fit_transform(y_train_balanced)\n",
    "input_shape = X_train_balanced.shape[1]\n",
    "num_classes = len(np.unique(y_train_balanced_encoded))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_balanced, y_train_balanced_encoded)):\n",
    "    print(f\"Training fold {fold+1}/{num_folds}...\")\n",
    "    X_train_fold, X_val_fold = X_train_balanced[train_idx], X_train_balanced[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_balanced_encoded[train_idx], y_train_balanced_encoded[val_idx]\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001), input_shape=(input_shape,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=epochs, batch_size=batch_size, validation_data=(X_val_fold, y_val_fold), verbose=1)\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    fold_losses.append(val_loss)\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "    print(f\"Fold {fold+1} - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Report cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(f\"Mean Validation Loss: {np.mean(fold_losses):.4f} ± {np.std(fold_losses):.4f}\")\n",
    "print(f\"Mean Validation Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "# Final Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(\"Test F1 Score:\", f1)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_classes))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuaFHhXnLMIy"
   },
   "source": [
    "full code excepte replace SMOTE with GANS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "c84Lh5gLo6bt",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"METABRIC_RNA_Mutation.csv\", low_memory=False)\n",
    "\n",
    "# Data Cleaning: Drop specified columns and encode target variable\n",
    "columns_to_drop = [\n",
    "    \"age_at_diagnosis\", \"tumor_other_histologic_subtype\", \"inferred_menopausal_state\",\n",
    "    \"mutation_count\", \"hormone_therapy\", \"patient_id\", \"cellularity\",\n",
    "    \"type_of_breast_surgery\", \"cohort\", \"chemotherapy\", \"tumor_size\",\n",
    "    \"hormone_therapy\", \"menopausal_state\", \"overall_survival_months\",\n",
    "    \"overall_survival\", \"death_from_cancer\", \"tumor_stage\", \"er_status\",\n",
    "    \"pam50_+_claudin-low_subtype\", \"pr_status\", \"er_status_measured_by_ihc\",\n",
    "    \"nottingham_prognostic_index\", \"tumor_stage\", \"death_from_cancer\",\n",
    "    \"integrative_cluster\", \"her2_status_measured_by_snp6\", \"her2_status\",\n",
    "    \"radio_therapy\", \"3-gene_classifier_subtype\", \"oncotree_code\"\n",
    "]\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"cancer_type_detailed\"])\n",
    "\n",
    "# One-Hot Encoding & Imputation\n",
    "X = pd.get_dummies(data.drop(columns=[\"cancer_type_detailed\"]), drop_first=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Feature Selection with Lasso\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_scaled, y)\n",
    "selected_featuresLasso = X.columns[(lasso.coef_ != 0)]\n",
    "X_selected = X_scaled[:, lasso.coef_ != 0]\n",
    "\n",
    "# Autoencoder for Dimensionality Reduction\n",
    "input_dim = X_selected.shape[1]\n",
    "encoding_dim = 50\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_selected, X_selected, epochs=50, batch_size=32, shuffle=True, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Extract the encoder part to get the compressed representation\n",
    "encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "X_encoded = encoder_model.predict(X_selected)\n",
    "\n",
    "# Class Filtering: Remove classes with fewer than 2 instances\n",
    "class_counts = Counter(y)\n",
    "single_instance_classes = [cls for cls, count in class_counts.items() if count < 2]\n",
    "mask = ~np.isin(y, single_instance_classes)\n",
    "X_filtered = X_encoded[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "print(X_filtered.shape)\n",
    "\n",
    "# Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.3, stratify=y_filtered, random_state=42)\n",
    "\n",
    "# Define the GAN\n",
    "latent_dim = 100  # Size of the noise vector\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "def build_generator(latent_dim, num_features):\n",
    "    # Create a LeakyReLU object\n",
    "    leaky_relu = LeakyReLU(alpha=0.2)\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=(latent_dim,)),\n",
    "        Dense(128, activation=leaky_relu),  # Use leaky_relu as activation\n",
    "        Dense(256, activation=leaky_relu),  # Use leaky_relu as activation\n",
    "        Dense(512, activation=leaky_relu),  # Use leaky_relu as activation\n",
    "        Dense(num_features, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator(num_features):\n",
    "    model = Sequential([\n",
    "        Input(shape=(num_features,)),\n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.2),  # Changed 'negative_slope' to 'alpha'\n",
    "        Dropout(0.3),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),  # Changed 'negative_slope' to 'alpha'\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(latent_dim,))\n",
    "    generated_data = generator(gan_input)\n",
    "    gan_output = discriminator(generated_data)\n",
    "    gan = Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "    return gan\n",
    "\n",
    "generator = build_generator(latent_dim, num_features)\n",
    "discriminator = build_discriminator(num_features)\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "def train_gan(generator, discriminator, gan, X_train_minority, epochs=100, batch_size=128):\n",
    "    half_batch = batch_size // 2\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, X_train_minority.shape[0], half_batch)\n",
    "        real_samples = X_train_minority[idx]\n",
    "        real_labels = np.ones((half_batch, 1))\n",
    "        d_loss_real = discriminator.train_on_batch(real_samples, real_labels)\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "        fake_samples = generator.predict(noise)\n",
    "        fake_labels = np.zeros((half_batch, 1))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_labels = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "        d_loss_real_value = d_loss_real[0] if isinstance(d_loss_real, (list, tuple)) else d_loss_real\n",
    "        d_loss_fake_value = d_loss_fake[0] if isinstance(d_loss_fake, (list, tuple)) else d_loss_fake\n",
    "        g_loss_value = g_loss[0] if isinstance(g_loss, (list, tuple)) else g_loss\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"{epoch}/{epochs} [D loss: {0.5 * (d_loss_real_value + d_loss_fake_value):.4f}] [G loss: {g_loss_value:.4f}]\")\n",
    "\n",
    "# Identify minority class\n",
    "minority_class = 1  # Replace with actual minority class\n",
    "X_train_minority = X_train[y_train == minority_class]\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, gan, X_train_minority)\n",
    "\n",
    "# Generate synthetic samples\n",
    "num_synthetic_samples = X_train_minority.shape[0]\n",
    "noise = np.random.normal(0, 1, (num_synthetic_samples, latent_dim))\n",
    "synthetic_samples = generator.predict(noise)\n",
    "\n",
    "# Combine synthetic samples with original training data\n",
    "X_train_balanced = np.vstack((X_train, synthetic_samples))\n",
    "y_train_balanced = np.hstack((y_train, [minority_class] * num_synthetic_samples))\n",
    "\n",
    "# Check the shape of the dataset after adding synthetic samples\n",
    "print(\"Shape of X_train_balanced:\", X_train_balanced.shape)\n",
    "print(\"Shape of y_train_balanced:\", y_train_balanced.shape)\n",
    "\n",
    "# Check the class distribution after GANs\n",
    "class_distribution = Counter(y_train_balanced)\n",
    "print(\"Class distribution after GANs:\", class_distribution)\n",
    "\n",
    "\n",
    "# Cross-validation Training\n",
    "num_folds = 5\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 0.0005\n",
    "y_train_balanced_encoded = LabelEncoder().fit_transform(y_train_balanced)\n",
    "input_shape = X_train_balanced.shape[1]\n",
    "num_classes = len(np.unique(y_train_balanced_encoded))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_balanced, y_train_balanced_encoded)):\n",
    "    print(f\"Training fold {fold+1}/{num_folds}...\")\n",
    "    X_train_fold, X_val_fold = X_train_balanced[train_idx], X_train_balanced[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_balanced_encoded[train_idx], y_train_balanced_encoded[val_idx]\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001), input_shape=(input_shape,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=epochs, batch_size=batch_size, validation_data=(X_val_fold, y_val_fold), verbose=1)\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    fold_losses.append(val_loss)\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "    print(f\"Fold {fold+1} - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Report cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(f\"Mean Validation Loss: {np.mean(fold_losses):.4f} ± {np.std(fold_losses):.4f}\")\n",
    "print(f\"Mean Validation Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "# Final Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(\"Test F1 Score:\", f1)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_classes))\n",
    "\n",
    "\n",
    "# Check the shape of the test set\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "# Check the class distribution in the test set\n",
    "class_distribution_test = Counter(y_test)\n",
    "print(\"Class distribution in y_test:\", class_distribution_test)\n",
    "\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_encoded, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
